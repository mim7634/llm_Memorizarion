# 2_train_model.py

import torch
from transformers import AutoTokenizer, GPT2LMHeadModel, GPT2Config, DefaultDataCollator
from datasets import load_dataset
from torch.utils.data import DataLoader
from torch.optim import AdamW
from tqdm import tqdm
import os
import datetime
import torch.nn.functional as F

class TRAIN_MODEL:
    def __init__(self, ppl_target=1.01, num_epochs=10000, learning_rate=1e-5):
        # --- ãƒ‘ã‚¹ã¨è¨­å®š ---
        self.DATA_DIR = "data"
        self.MODEL_BASE_DIR = "model" 
        self.TRAIN_PATH = os.path.join(self.DATA_DIR, "train_data.txt")

        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.BATCH_SIZE = 8
        self.NUM_EPOCHS = num_epochs
        self.LEARNING_RATE = learning_rate
        self.PPL_TARGET = ppl_target
        self.NUM_PROCS = 4 
        
        # è¨“ç·´ä¸­ã«ä½¿ç”¨ã™ã‚‹å¤‰æ•°
        self.final_epoch = 0 
        self.avg_loss = 0
        self.perplexity = 0
        self.current_run_num, self.today_date = self._get_model_save_dir_prefix()
        # model_folder_nameã‚’æ ¼ç´ã™ã‚‹ãŸã‚ã®å±æ€§
        self.model_folder_name = "" 

    def _get_model_save_dir_prefix(self):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®é€£ç•ªã¨æ—¥ä»˜ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ (ValueErrorä¿®æ­£æ¸ˆã¿)"""
        today_date = datetime.date.today().strftime("%Y%m%d")
        os.makedirs(self.MODEL_BASE_DIR, exist_ok=True)
        
        existing_folders = [d for d in os.listdir(self.MODEL_BASE_DIR) if os.path.isdir(os.path.join(self.MODEL_BASE_DIR, d))]
        latest_num = 0
        
        if existing_folders:
            # ãƒ•ã‚©ãƒ«ãƒ€åã®æœ€åˆã®2æ–‡å­—ãŒæ•°å€¤ã§ã‚ã‚‹ã‚‚ã®ã ã‘ã‚’æŠ½å‡ºã—ã€æœ€å¤§å€¤ã‚’å–å¾—
            numeric_prefixes = [int(f[:2]) for f in existing_folders if f[:2].isdigit()]
            
            # ãƒªã‚¹ãƒˆãŒç©ºã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯
            if numeric_prefixes: 
                latest_num = max(numeric_prefixes)

        current_run_num = latest_num + 1
        return current_run_num, today_date 

    def _tokenize_function(self, examples, tokenizer):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚’è¡Œã†é–¢æ•°"""
        # (1) é€šå¸¸ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å‡¦ç†
        tokenized_input = tokenizer(examples["text"], truncation=True, padding="max_length", max_length=64)
        tokenized_input["labels"] = tokenized_input["input_ids"].copy()

        # === ğŸ”½ ãƒ‡ãƒãƒƒã‚°ç”¨ã®è¿½åŠ ã‚³ãƒ¼ãƒ‰ ğŸ”½ ===
        if self.final_epoch == 0 and "debug_flag" not in self.__dict__:
            self.debug_flag = True # 1å›ã ã‘å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ•ãƒ©ã‚°

            print("\n--- ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®ãƒ‡ãƒãƒƒã‚°å‡ºåŠ› ---")
            
            # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
            first_text = examples["text"][0]
            print(f"å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ: '{first_text}'")

            # ãƒˆãƒ¼ã‚¯ãƒ³ID (IDã®ãƒªã‚¹ãƒˆ)
            first_ids = tokenized_input["input_ids"][0]
            print(f"ãƒˆãƒ¼ã‚¯ãƒ³ID (æŠœç²‹): {first_ids[:10]}...")

            # ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰æ–‡å­—åˆ—ï¼‰
            # `convert_ids_to_tokens` ã§ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã‚’ç¢ºèªã§ãã¾ã™
            first_tokens = tokenizer.convert_ids_to_tokens(first_ids)
            # [PAD]ã‚’é™¤å¤–ã—ã¦è¡¨ç¤º
            actual_tokens = [t for t in first_tokens if t != tokenizer.pad_token]
            
            print(f"ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²: {actual_tokens}")
            print("----------------------------------------\n")
        # === ğŸ”¼ ãƒ‡ãƒãƒƒã‚°ç”¨ã®è¿½åŠ ã‚³ãƒ¼ãƒ‰ ğŸ”¼ ===

        return tokenized_input

    def run_training(self):
        # --- 1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã®æº–å‚™ ---
        print("1. ãƒ‡ãƒ¼ã‚¿ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æº–å‚™ä¸­...")
        tokenizer = AutoTokenizer.from_pretrained("gpt2") 
        
        # ğŸš¨ ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ <SEP> ã®è¿½åŠ ã¨èªè­˜ ğŸš¨
        special_tokens_dict = {
            'pad_token': '[PAD]',
            # <SEP> ã‚’è¿½åŠ ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦ç™»éŒ²ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€<SEP>ãŒå˜ä¸€ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦æ‰±ã‚ã‚Œã‚‹ã€‚
            'additional_special_tokens': ['<SEP>'] 
        }
        num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
        print(f"   ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ '{special_tokens_dict['additional_special_tokens'][0]}' ã‚’è¿½åŠ ã—ã¾ã—ãŸ (åˆè¨ˆ {num_added_toks} å€‹ã®æ–°è¦ãƒˆãƒ¼ã‚¯ãƒ³)ã€‚")

        # [PAD] ãƒˆãƒ¼ã‚¯ãƒ³ã®IDã‚’ç¢ºå®Ÿã«è¿½åŠ 
        if tokenizer.pad_token is None:
            tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        
        raw_datasets = load_dataset("text", data_files={"train": self.TRAIN_PATH})
        tokenized_datasets = raw_datasets.map(
            lambda examples: self._tokenize_function(examples, tokenizer), 
            batched=True, 
            num_proc=self.NUM_PROCS, 
            remove_columns=["text"]
        )
        
        data_collator = DefaultDataCollator()

        train_dataloader = DataLoader(
            tokenized_datasets["train"], 
            batch_size=self.BATCH_SIZE,
            collate_fn=data_collator 
        )

        # --- 2. ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã®å®šç¾© ---
        print("2. å°è¦æ¨¡Transformerãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ä¸­...")
        MODEL_CONFIG = GPT2Config(
            # èªå½™ã‚µã‚¤ã‚ºã‚’æ›´æ–°å¾Œã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èªå½™ã‚µã‚¤ã‚ºã«åˆã‚ã›ã‚‹
            vocab_size=len(tokenizer),
            n_layer=4, n_head=8, n_embd=256,
            pad_token_id=tokenizer.pad_token_id,
            embd_pdrop=0.0, attn_pdrop=0.0, resid_pdrop=0.0 
        )

        model = GPT2LMHeadModel(MODEL_CONFIG)
        
        # ğŸš¨ ãƒ¢ãƒ‡ãƒ«ã®åŸ‹ã‚è¾¼ã¿å±¤ã‚’æ›´æ–°å¾Œã®èªå½™ã‚µã‚¤ã‚ºã«åˆã‚ã›ã‚‹ ğŸš¨
        model.resize_token_embeddings(len(tokenizer)) 
        
        num_params = sum(p.numel() for p in model.parameters())
        print(f"   ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {num_params:,}")
        model.to(self.DEVICE)
        optimizer = AdamW(model.parameters(), lr=self.LEARNING_RATE)

        # --- 3. è¨“ç·´ãƒ«ãƒ¼ãƒ— ---
        print(f"3. è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™ã€‚ç›®æ¨™PPL: {self.PPL_TARGET}")
        model.train()
        
        for epoch in range(1, self.NUM_EPOCHS + 1):
            total_loss = 0
            for batch in tqdm(train_dataloader, desc=f"Epoch {epoch}/{self.NUM_EPOCHS}"):
                batch = {k: v.to(self.DEVICE) for k, v in batch.items()}
                
                outputs = model(**batch)
                loss = outputs.loss
                
                loss.backward()
                optimizer.step()
                optimizer.zero_grad()
                
                total_loss += loss.item()
                
            self.avg_loss = total_loss / len(train_dataloader)
            self.perplexity = torch.exp(torch.tensor(self.avg_loss)).item()
            print(f"Epoch {epoch} å®Œäº†. è¨“ç·´æå¤±: {self.avg_loss:.6f}, PPL: {self.perplexity:.4f}")
            
            # PPLç›®æ¨™ã«é”ã—ãŸã‚‰è¨“ç·´ã‚’çµ‚äº†
            if self.perplexity < self.PPL_TARGET and epoch > 10:
                self.final_epoch = epoch
                print(f"\n--- ç›®æ¨™é”æˆ ---")
                print(f"PPL {self.perplexity:.4f} ã«åˆ°é”ã—ãŸãŸã‚ã€è¨“ç·´ã‚’çµ‚äº†ã—ã¾ã™ (æœ€çµ‚ã‚¨ãƒãƒƒã‚¯: {self.final_epoch})ã€‚")
                break
            
            self.final_epoch = epoch 

        # --- 4. è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ ---
        self._save_model(model, tokenizer)
    
    def _save_model(self, model, tokenizer):
        """ãƒ¢ãƒ‡ãƒ«ã‚’å‘½åè¦å‰‡ã«å¾“ã£ã¦ä¿å­˜ã™ã‚‹"""
        
        rounded_ppl = round(self.perplexity, 4)

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã®ç”Ÿæˆ
        model_folder_name = (
            f"{self.current_run_num:02d}_{self.today_date}" 
            f"_epc_{self.final_epoch}"
            f"_ppl_{str(rounded_ppl).replace('.', '-')}" 
            f"_llm"
        )
        MODEL_DIR = os.path.join(self.MODEL_BASE_DIR, model_folder_name)
        
        os.makedirs(MODEL_DIR, exist_ok=True)
        model.save_pretrained(MODEL_DIR)
        tokenizer.save_pretrained(MODEL_DIR)
        
        # ã‚¯ãƒ©ã‚¹å±æ€§ã«ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€åã‚’è¨˜éŒ²
        self.model_folder_name = model_folder_name
        
        print(f"\n--- è¨“ç·´å®Œäº† ---")
        print(f"ãƒ¢ãƒ‡ãƒ«ã¯æ­£å¸¸ã« '{MODEL_DIR}' ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
        print(f"â˜…æœ€çµ‚ã‚¨ãƒãƒƒã‚¯æ•°: {self.final_epoch} â˜…")

# --- ãƒ¡ã‚¤ãƒ³ã‚¬ãƒ¼ãƒ‰ ---
if __name__ == '__main__':
    # ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã¨å®Ÿè¡Œ
    # PPLç›®æ¨™: 1.01
    trainer = TRAIN_MODEL(num_epochs=10) 
    trainer.run_training()
    print(trainer.model_folder_name)