import torch
from transformers import AutoTokenizer, GPT2LMHeadModel, GPT2Config, DefaultDataCollator
from datasets import load_dataset
from torch.utils.data import DataLoader
from torch.optim import AdamW
from tqdm import tqdm
import os
import datetime
import torch.nn.functional as F

class TRAIN_MODEL:
    # resume_from_dir ã¯ run_training ã®å¼•æ•°ã«ã™ã‚‹ãŸã‚ã€__init__ã‹ã‚‰ã¯å‰Šé™¤
    def __init__(self, ppl_stop=True, ppl_target=1.01, num_epochs=10000, learning_rate=1e-5, snapshot_interval=50, model_basa_dir='model', n_layer=4, n_head=8, n_embd=256):
        # --- ãƒ‘ã‚¹ã¨è¨­å®š ---
        self.DATA_DIR = "data"
        self.MODEL_BASE_DIR = model_basa_dir
        self.TRAIN_PATH = os.path.join(self.DATA_DIR, "train_data.txt")

        self.n_layer=n_layer
        self.n_head=n_head
        self.n_embd=n_embd

        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.BATCH_SIZE = 8
        self.NUM_EPOCHS = num_epochs
        self.LEARNING_RATE = learning_rate
        self.PPL_TARGET = ppl_target
        self.NUM_PROCS = 4 
        
        # è¨“ç·´ä¸­ã«ä½¿ç”¨ã™ã‚‹å¤‰æ•°
        self.ppl_stop = ppl_stop
        self.final_epoch = 0 
        self.avg_loss = 0
        self.perplexity = 0
        self.current_run_num, self.today_date = self._get_model_save_dir_prefix()
        self.model_folder_name = "" 

        self.snapshot_interval = snapshot_interval

    def _get_model_save_dir_prefix(self):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®é€£ç•ªã¨æ—¥ä»˜ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆã™ã‚‹"""
        today_date = datetime.date.today().strftime("%Y%m%d")
        os.makedirs(self.MODEL_BASE_DIR, exist_ok=True)
        
        existing_folders = [d for d in os.listdir(self.MODEL_BASE_DIR) if os.path.isdir(os.path.join(self.MODEL_BASE_DIR, d))]
        latest_num = 0
        
        if existing_folders:
            numeric_prefixes = [int(f[:2]) for f in existing_folders if f[:2].isdigit()]
            if numeric_prefixes: 
                latest_num = max(numeric_prefixes)

        current_run_num = latest_num + 1
        return current_run_num, today_date 

    def _tokenize_function(self, examples, tokenizer):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚’è¡Œã†é–¢æ•°"""
        tokenized_input = tokenizer(examples["text"], truncation=True, padding="max_length", max_length=64)
        tokenized_input["labels"] = tokenized_input["input_ids"].copy()
        return tokenized_input

    def run_training(self, resume_from_dir=None):
        # --- 1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã®æº–å‚™ ---
        print("1. ãƒ‡ãƒ¼ã‚¿ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æº–å‚™ä¸­...")
        
        # ğŸš¨ ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ <SEP> ã®è¿½åŠ ã¨èªè­˜ ğŸš¨
        tokenizer = AutoTokenizer.from_pretrained("gpt2") 
        special_tokens_dict = {
            'pad_token': '[PAD]',
            'additional_special_tokens': ['<SEP>'] 
        }
        num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
        print(f"   ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ '{special_tokens_dict['additional_special_tokens'][0]}' ã‚’è¿½åŠ ã—ã¾ã—ãŸ (åˆè¨ˆ {num_added_toks} å€‹ã®æ–°è¦ãƒˆãƒ¼ã‚¯ãƒ³)ã€‚")

        if tokenizer.pad_token is None:
            tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        
        raw_datasets = load_dataset("text", data_files={"train": self.TRAIN_PATH})
        tokenized_datasets = raw_datasets.map(
            lambda examples: self._tokenize_function(examples, tokenizer), 
            batched=True, 
            num_proc=self.NUM_PROCS, 
            remove_columns=["text"]
        )
        
        data_collator = DefaultDataCollator()

        train_dataloader = DataLoader(
            tokenized_datasets["train"], 
            batch_size=self.BATCH_SIZE,
            collate_fn=data_collator 
        )

        # --- 2. ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©ã¨ãƒ­ãƒ¼ãƒ‰ ---
        print("2. å°è¦æ¨¡Transformerãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ä¸­...")
        start_epoch = 1
        
        if resume_from_dir:
            # â˜…â˜…â˜… æ—¢å­˜ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ­ãƒ¼ãƒ‰ â˜…â˜…â˜…
            CHECKPOINT_DIR = os.path.join(self.MODEL_BASE_DIR, resume_from_dir)
            print(f"   [RESUME] æ—¢å­˜ãƒ¢ãƒ‡ãƒ« '{CHECKPOINT_DIR}' ã‹ã‚‰å­¦ç¿’ã‚’å†é–‹ã—ã¾ã™ã€‚")
            
            # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
            model = GPT2LMHeadModel.from_pretrained(CHECKPOINT_DIR).to(self.DEVICE)
            
            # ãƒ¢ãƒ‡ãƒ«ã®Configã¯ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«æº–æ‹ ã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯Configã®æ–°è¦å®šç¾©ã‚’ã‚¹ã‚­ãƒƒãƒ—

            # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®å®šç¾© (ãƒ­ãƒ¼ãƒ‰å‰ã«å®šç¾©ãŒå¿…è¦)
            optimizer = AdamW(model.parameters(), lr=self.LEARNING_RATE)
            
            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ­ãƒ¼ãƒ‰
            checkpoint_path = os.path.join(CHECKPOINT_DIR, 'optimizer_checkpoint.pt')
            if os.path.exists(checkpoint_path):
                checkpoint_data = torch.load(checkpoint_path, map_location=self.DEVICE)
                optimizer.load_state_dict(checkpoint_data['optimizer_state_dict'])
                start_epoch = checkpoint_data['epoch'] + 1
                self.final_epoch = checkpoint_data['epoch'] # è¨“ç·´ãŒä¸­æ–­ã—ãŸã‚¨ãƒãƒƒã‚¯ã‚’è¨˜éŒ²
                print(f"   [RESUME] ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã¨ã‚¨ãƒãƒƒã‚¯çŠ¶æ…‹ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚å†é–‹ã‚¨ãƒãƒƒã‚¯: {start_epoch}")
            else:
                print("   [WARNING] optimizer_checkpoint.pt ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¨ãƒãƒƒã‚¯1ã‹ã‚‰å­¦ç¿’ã‚’å†é–‹ã—ã¾ã™ã€‚")

            # è¨“ç·´å†é–‹æ™‚ã®é€£ç•ªã¨æ—¥ä»˜ã‚’ãƒ­ãƒ¼ãƒ‰å…ƒãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰å†æ§‹ç¯‰
            parts = resume_from_dir.split('_')
            self.current_run_num = int(parts[0])
            self.today_date = parts[1]
            self.model_folder_name = resume_from_dir
            
        else:
            # â˜…â˜…â˜… æ–°è¦è¨“ç·´ â˜…â˜…â˜…
            MODEL_CONFIG = GPT2Config(
                vocab_size=len(tokenizer),
                n_layer=self.n_layer, n_head=self.n_head, n_embd=self.n_embd,
                pad_token_id=tokenizer.pad_token_id,
                embd_pdrop=0.0, attn_pdrop=0.0, resid_pdrop=0.0 
            )
            model = GPT2LMHeadModel(MODEL_CONFIG).to(self.DEVICE)
            # èªå½™ã‚µã‚¤ã‚ºã¯æ–°è¦è¨“ç·´ã§ã‚‚å¿…ãšæ›´æ–°
            model.resize_token_embeddings(len(tokenizer)) 
            optimizer = AdamW(model.parameters(), lr=self.LEARNING_RATE)

        num_params = sum(p.numel() for p in model.parameters())
        print(f"   ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {num_params:,}")

        # --- 3. è¨“ç·´ãƒ«ãƒ¼ãƒ— ---
        print(f"3. è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™ã€‚ç›®æ¨™PPL: {self.PPL_TARGET}, é–‹å§‹ã‚¨ãƒãƒƒã‚¯: {start_epoch}")
        model.train()
        
        for epoch in range(start_epoch, self.NUM_EPOCHS + 1):
            total_loss = 0
            # å†…éƒ¨ã®ãƒ‡ãƒãƒƒã‚°ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ (å†é–‹æ™‚ã‚‚ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ã¯ã‚¹ã‚­ãƒƒãƒ—)
            if "debug_flag" in self.__dict__:
                 del self.debug_flag 
                 
            for batch in tqdm(train_dataloader, desc=f"Epoch {epoch}/{self.NUM_EPOCHS}"):
                batch = {k: v.to(self.DEVICE) for k, v in batch.items()}
                
                outputs = model(**batch)
                loss = outputs.loss
                
                loss.backward()
                optimizer.step()
                optimizer.zero_grad()
                
                total_loss += loss.item()
                
            self.avg_loss = total_loss / len(train_dataloader)
            self.perplexity = torch.exp(torch.tensor(self.avg_loss)).item()
            self.final_epoch = epoch # å¸¸ã«ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯ã‚’è¨˜éŒ²
            print(f"Epoch {epoch} å®Œäº†. è¨“ç·´æå¤±: {self.avg_loss:.6f}, PPL: {self.perplexity:.4f}")
            
            # â˜…â˜…â˜… å®šæœŸçš„ãªã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä¿å­˜ â˜…â˜…â˜…
            if self.snapshot_interval > 0 and (epoch % self.snapshot_interval == 0):
                # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’å¼•æ•°ã¨ã—ã¦æ¸¡ã—ã€is_final=Falseã§ä¿å­˜
                self._save_model(model, tokenizer, optimizer, is_final=False)

            # PPLç›®æ¨™ã«é”ã—ãŸã‚‰è¨“ç·´ã‚’çµ‚äº†
            if self.ppl_stop and (self.perplexity < self.PPL_TARGET and epoch > 10):
                print(f"\n--- ç›®æ¨™é”æˆ ---")
                print(f"PPL {self.perplexity:.4f} ã«åˆ°é”ã—ãŸãŸã‚ã€è¨“ç·´ã‚’çµ‚äº†ã—ã¾ã™ (æœ€çµ‚ã‚¨ãƒãƒƒã‚¯: {self.final_epoch})ã€‚")
                break
            
        # --- 4. è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ ---
        # è¨“ç·´ãŒNUM_EPOCHSã¾ã§å®Œäº†ã—ãŸã‹ã€PPLã§åœæ­¢ã—ãŸå ´åˆã€æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        self._save_model(model, tokenizer, optimizer, is_final=True)


    def _save_model(self, model, tokenizer, optimizer, is_final=True):
        """ãƒ¢ãƒ‡ãƒ«ã¨Optimizerã®çŠ¶æ…‹ã‚’å‘½åè¦å‰‡ã«å¾“ã£ã¦ä¿å­˜ã™ã‚‹"""
        
        rounded_ppl = round(self.perplexity, 4)

        # ãƒ•ã‚©ãƒ«ãƒ€åã®æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯ã‚’å¤‰æ›´ (ç¾åœ¨ã®é€£ç•ªã¨æ—¥ä»˜ã‚’ä½¿ç”¨)
        if is_final:
            model_folder_name = (
                f"{self.current_run_num:02d}_{self.today_date}" 
                f"_epc_{self.final_epoch}"
                f"_ppl_{str(rounded_ppl).replace('.', '-')}" 
                f"_llm"
            )
        else:
            # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆåã¯ãƒªã‚«ãƒãƒªã‚’å®¹æ˜“ã«ã™ã‚‹ãŸã‚ã€é€£ç•ªã¨ã‚¨ãƒãƒƒã‚¯ã§å‘½å
            model_folder_name = (
                f"{self.current_run_num:02d}_{self.today_date}_snapshot_epc_{self.final_epoch}"
            )
            
        MODEL_DIR = os.path.join(self.MODEL_BASE_DIR, model_folder_name)
        
        os.makedirs(MODEL_DIR, exist_ok=True)
        model.save_pretrained(MODEL_DIR)
        tokenizer.save_pretrained(MODEL_DIR)
        
        # é‡è¦ãªè¿½åŠ : Optimizerã®çŠ¶æ…‹ã¨ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯æ•°ã‚’ä¿å­˜
        checkpoint_data = {
            'epoch': self.final_epoch,
            'optimizer_state_dict': optimizer.state_dict(),
        }
        torch.save(checkpoint_data, os.path.join(MODEL_DIR, 'optimizer_checkpoint.pt'))
        
        if is_final:
            self.model_folder_name = model_folder_name
            print(f"\n--- æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº† ---")
            print(f"ãƒ¢ãƒ‡ãƒ«ã¯æ­£å¸¸ã« '{MODEL_DIR}' ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
            print(f"â˜…æœ€çµ‚ã‚¨ãƒãƒƒã‚¯æ•°: {self.final_epoch} â˜…")
        else:
            print(f"\n--- ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä¿å­˜å®Œäº† ---")
            print(f"é€”ä¸­çµŒéã‚’ '{MODEL_DIR}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")


# --- ãƒ¡ã‚¤ãƒ³ã‚¬ãƒ¼ãƒ‰ ---
if __name__ == '__main__':
    # ä¾‹ï¼šæ–°è¦è¨“ç·´
    # trainer = TRAIN_MODEL(num_epochs=700, snapshot_interval=50) 
    # trainer.run_training()
    
    # ä¾‹ï¼šå­¦ç¿’å†é–‹
    # trainer = TRAIN_MODEL(num_epochs=1000, snapshot_interval=50) 
    # trainer.run_training(resume_from_dir='01_20251020_snapshot_epc_500')
    
    # â˜…â˜…â˜… ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢å®Ÿé¨“ã®ãŸã‚ã®å®Ÿè¡Œ â˜…â˜…â˜…
    trainer = TRAIN_MODEL(num_epochs=50000, snapshot_interval=1000, ppl_stop=False, n_embd=8) 
    trainer.run_training()
    print(trainer.model_folder_name)